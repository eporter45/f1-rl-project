{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b7224-d55e-4665-ba36-cf9f72922b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class F1RaceEnv(gym.Env):\n",
    "    def __init__(self, race_data):\n",
    "        super(F1RaceEnv, self).__init__()\n",
    "        \n",
    "        self.race_data = race_data\n",
    "        \n",
    "        # Number of drivers based on lap time matrix\n",
    "        self.num_drivers = self.race_data['lap_times'].shape[1]\n",
    "        \n",
    "        # State space:\n",
    "        # [Lap, Position, Gap to Leader, Gap to Car Ahead, Tyre Type, Tyre Age, Pit Stop History]\n",
    "        self.state_shape = (self.num_drivers, 7)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=self.state_shape, dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Action Space:\n",
    "        # Pace Strategy (3) x Pit Decision (2) x Tire Choice (3) = 18 discrete actions\n",
    "        self.action_space = spaces.Discrete(18)\n",
    "        \n",
    "        # Internal state tracking\n",
    "        self.current_lap = 0\n",
    "        self.state = None\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment at the start of the race.\n",
    "        \"\"\"\n",
    "        self.current_lap = 1\n",
    "        \n",
    "        # Get lap data for the first lap\n",
    "        lap_times = self.race_data['lap_times'].iloc[self.current_lap - 1].values\n",
    "        gaps_to_leader = self.race_data['gaps_to_leader'].iloc[self.current_lap - 1].values\n",
    "        gaps_to_car_ahead = self.race_data['gaps_to_car_ahead'].iloc[self.current_lap - 1].values\n",
    "        \n",
    "        # Initialize state tensor (positions based on lap times)\n",
    "        self.state = np.zeros(self.state_shape)\n",
    "        \n",
    "        for i in range(self.num_drivers):\n",
    "            self.state[i] = [\n",
    "                self.current_lap,                  # Lap number\n",
    "                i + 1,                             # Track position\n",
    "                gaps_to_leader[i],                 # Gap to leader\n",
    "                gaps_to_car_ahead[i],              # Gap to car ahead\n",
    "                0,                                 # Tyre Type (Soft = 0, Medium = 1, Hard = 2)\n",
    "                0,                                 # Tyre Age (0 at race start)\n",
    "                0                                  # Pit Stop History (0 = no stop yet)\n",
    "            ]\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Step the environment forward by one lap.\n",
    "        \"\"\"\n",
    "        # Decode the action\n",
    "        pace_strategy = action // 6\n",
    "        pit_decision = (action % 6) // 3\n",
    "        tyre_choice = action % 3\n",
    "        \n",
    "        # Example of how action affects the race:\n",
    "        # - Higher pace = faster lap time but higher degradation\n",
    "        # - Pit decision = lose time but reset tyre age\n",
    "        # - Tyre choice = affects future lap times\n",
    "\n",
    "        for i in range(self.num_drivers):\n",
    "            if pit_decision == 1:\n",
    "                # If the driver pits â†’ Reset tyre age and change tyre type\n",
    "                self.state[i][5] = 0  # Reset tyre age\n",
    "                self.state[i][4] = tyre_choice  # Change tyre type\n",
    "                self.state[i][6] += 1  # Increment pit stop count\n",
    "            else:\n",
    "                # Increase tyre age\n",
    "                self.state[i][5] += 1\n",
    "            \n",
    "            # Adjust pace based on tyre type and age\n",
    "            if pace_strategy == 0:  # Push\n",
    "                lap_penalty = 0.1 * self.state[i][5]\n",
    "            elif pace_strategy == 1:  # Defend\n",
    "                lap_penalty = -0.05 * self.state[i][5]\n",
    "            else:  # Manage\n",
    "                lap_penalty = 0\n",
    "            \n",
    "            # Simulate lap time increase based on degradation\n",
    "            self.state[i][2] += lap_penalty\n",
    "\n",
    "        # Increment lap count\n",
    "        self.current_lap += 1\n",
    "\n",
    "        # Check if race is over\n",
    "        done = self.current_lap > len(self.race_data['lap_times'])\n",
    "\n",
    "        # Reward function:\n",
    "        # - Higher position = higher reward\n",
    "        # - Successful undercut = bonus\n",
    "        # - Tyre degradation penalty\n",
    "        reward = 0\n",
    "        for i in range(self.num_drivers):\n",
    "            position = self.state[i][1]\n",
    "            reward += (self.num_drivers - position) * 0.1\n",
    "            \n",
    "            if pit_decision == 1:\n",
    "                if i > 0 and self.state[i][2] < self.state[i - 1][2]:\n",
    "                    reward += 0.5  # Successful undercut\n",
    "                else:\n",
    "                    reward -= 0.3  # Pit failure\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Render the current race state.\n",
    "        \"\"\"\n",
    "        print(f\"\\nLap: {self.current_lap}\")\n",
    "        print(\"Pos | Gap to Leader | Gap to Car Ahead | Tyre Type | Tyre Age | Pit Stops\")\n",
    "        for i in range(self.num_drivers):\n",
    "            print(f\"{int(self.state[i][1]):3} | {self.state[i][2]:12.3f} | {self.state[i][3]:14.3f} |\"\n",
    "                  f\" {int(self.state[i][4]):9} | {int(self.state[i][5]):8} | {int(self.state[i][6]):9}\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Example setup\n",
    "example_race = races_cleaned[list(races_cleaned.keys())[0]]\n",
    "env = F1RaceEnv(example_race)\n",
    "state = env.reset()\n",
    "env.render()\n",
    "\n",
    "# Take a sample action\n",
    "sample_action = env.action_space.sample()\n",
    "new_state, reward, done, _ = env.step(sample_action)\n",
    "env.render()\n",
    "\n",
    "print(f\"\\nAction Taken: {sample_action}, Reward: {reward}, Done: {done}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
